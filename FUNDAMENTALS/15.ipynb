{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled33.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyJM49/TKgmwuhNMK1ZqPY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t1seo/AIFFEL/blob/master/FUNDAMENTALS/15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elvbP2ytX80B"
      },
      "source": [
        "# 15. 딥러닝 들여다보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0arzyu8bYT8T"
      },
      "source": [
        "## 15-1. 들어가며\n",
        "\n",
        "- 딥러닝 문제 구성에 대한 기본적인 이해를 높인다.\n",
        "- Neural Network에 사용되는 용어들에 대한 이해를 높인다.\n",
        "- 딥러닝 프레임워크를 사용하지 않고, Numpy만을 이용해 딥러닝 모델과 훈련 과정을 직접 구현해 본다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Alp80dCYa9c"
      },
      "source": [
        "## 15-2. 신경망 구성 (1) 개요\n",
        "\n",
        "- **신경망(Neural Network)**\n",
        "- **퍼셉트론(Perceptron)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3ux8QB0Yrvh"
      },
      "source": [
        "### MNIST Revisited"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJu7zRAcYuJJ",
        "outputId": "86fc1e3e-23c9-47f1-81b0-2ce075773d56"
      },
      "source": [
        "# Tensorflow 기반 분류\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# MNIST 데이터를 로드\n",
        "mnist = keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 모델에 맞게 데이터 가공\n",
        "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
        "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1] * x_train_norm.shape[2])\n",
        "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1] * x_test_norm.shape[2])\n",
        "print(x_train_norm.shape)\n",
        "print(x_train_reshaped.shape)\n",
        "\n",
        "# 딥러닝 모델 구성 - 2 Layer Peceptron\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784, ))) # 입력층 d=784, 은닉층 레이어 H=50\n",
        "model.add(keras.layers.Dense(10, activation='softmax')) # 출력층 레이어 K=10\n",
        "model.summary()\n",
        "\n",
        "# 모델 구성과 학습\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train_reshaped, y_train, epochs=10)\n",
        "\n",
        "# 모델 테스트 결과\n",
        "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
        "print(\"test_loss: {}\".format(test_loss))\n",
        "print(\"test_accuracy: {}\".format(test_accuracy))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000, 784)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 50)                39250     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                510       \n",
            "=================================================================\n",
            "Total params: 39,760\n",
            "Trainable params: 39,760\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4948 - accuracy: 0.8813\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2306 - accuracy: 0.9344\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1813 - accuracy: 0.9479\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1510 - accuracy: 0.9572\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1303 - accuracy: 0.9626\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1142 - accuracy: 0.9680\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1018 - accuracy: 0.9720\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0915 - accuracy: 0.9750\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0828 - accuracy: 0.9772\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0756 - accuracy: 0.9795\n",
            "313/313 - 0s - loss: 0.1050 - accuracy: 0.9669\n",
            "test_loss: 0.1049971804022789\n",
            "test_accuracy: 0.9668999910354614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skVWoRt7Z9f-"
      },
      "source": [
        "### 다층 퍼셉트론 Overview\n",
        "\n",
        "- [What is the role of the bias in neural networks?](https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks)\n",
        "- **다층 퍼셉트론(Multi-Layer Perceptron; MLP)**\n",
        "- **DNN(Deep Neural Network)**\n",
        "- **Fully-Connected Neural Network** -> MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XySlYE8a79P"
      },
      "source": [
        "### Parameters/Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZSrAMhlcsyT",
        "outputId": "c4aed1fa-5a70-4a88-c483-ed7d5be8ffb5"
      },
      "source": [
        "# 입력층의 모양(shape)\n",
        "print(x_train_reshaped.shape)\n",
        "\n",
        "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다\n",
        "X = x_train_reshaped[:5]\n",
        "print(X.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(5, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gaUnHwOdPnR",
        "outputId": "df60bb72-be11-401f-e1b2-70a2e95331f3"
      },
      "source": [
        "# MLP 기반 딥러닝 모델을 Numpy로 다시 만들어본다\n",
        "weight_init_std = 0.1\n",
        "input_size = 784\n",
        "hidden_size = 50\n",
        "\n",
        "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
        "W1 = weight_init_std * np.random.randn(input_size, hidden_size) # 가우시안 표준 정규 분포에서 난수 matrix array생성\n",
        "\n",
        "# 바이어스 파라미터 b를 생성하고 Zero로 초기화\n",
        "b1 = np.zeros(hidden_size)\n",
        "\n",
        "a1 = np.dot(X, W1) + b1 # 은닉층 출력\n",
        "\n",
        "print(W1.shape)\n",
        "print(b1.shape)\n",
        "print(a1.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 50)\n",
            "(50,)\n",
            "(5, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmr4yZGMd4ot",
        "outputId": "687fa004-a4f6-4adb-e409-d4b734535583"
      },
      "source": [
        "# 첫 번째 데이터의 은닉층 출력을 확인\n",
        "a1[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.40568981,  0.15020417,  0.47497495,  0.03525855,  0.53463739,\n",
              "       -0.69223711, -1.02827081, -0.66427713, -0.65970925, -0.84804427,\n",
              "       -1.70924481, -1.1731063 , -1.61316629, -0.03663305,  0.23086301,\n",
              "        0.19410678,  0.41699383, -0.41171827, -0.19711678,  1.3274777 ,\n",
              "       -0.26938213,  0.30773385,  1.52529566, -0.18763187,  0.14848796,\n",
              "       -1.35574837, -0.53211084,  0.28227038, -0.44606402, -1.64236562,\n",
              "        0.31380329, -0.39327461, -0.8045489 ,  0.34814073, -0.36791301,\n",
              "       -0.52171896, -0.26980426,  1.69966868, -2.26197246,  0.55425936,\n",
              "        1.1913375 , -0.08898262, -0.97574425,  1.40141454,  1.21526803,\n",
              "       -0.4414148 ,  0.25681239,  1.50017038, -0.29444604, -1.07834535])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH9oPvuseBHq"
      },
      "source": [
        "## 15-3. 신경망 구성 (2) 활성화 함수와 손실 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEwbIFPEfKrN"
      },
      "source": [
        "### 활성화 함수 (Activation Functions)\n",
        "\n",
        "- [Why must a nonlinear activation function be used in a backpropagation neural network?](https://stackoverflow.com/questions/9782071/why-must-a-nonlinear-activation-function-be-used-in-a-backpropagation-neural-net/54503251#54503251)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7v4k8h5fR4C"
      },
      "source": [
        "### 1. sigmoid\n",
        "\n",
        "sigmoid보다 ReLU 함수를 더 많이 사용하는 이유\n",
        "- [vanishing gradient](https://brunch.co.kr/@chris-song/39) 현상이 발생한다.\n",
        "- exp 함수 사용 시 비용이 크다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-g3CfGmhLWl",
        "outputId": "e0be7e7a-8104-47b7-efd0-4bd46803e897"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "z1 = sigmoid(a1)\n",
        "print(z1[0]) # sigmoid의 출력은 모든 element가 0에서 1사이"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.39994607 0.5374806  0.61656058 0.50881373 0.63056406 0.3335356\n",
            " 0.26341948 0.33977947 0.34080493 0.29984328 0.15326169 0.23629397\n",
            " 0.16614948 0.49084276 0.55746077 0.5483749  0.60276368 0.39850019\n",
            " 0.45087975 0.79042311 0.43305879 0.57633202 0.82131697 0.45322917\n",
            " 0.53705393 0.20493217 0.3700247  0.57010275 0.39029699 0.16214343\n",
            " 0.57781333 0.40292925 0.30905331 0.58616664 0.40904541 0.37245037\n",
            " 0.43295515 0.84549146 0.09432174 0.63512323 0.76698019 0.47776901\n",
            " 0.27373703 0.80240826 0.77122974 0.3914039  0.56385255 0.81759989\n",
            " 0.42691575 0.25381927]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rihv4zfbhVok"
      },
      "source": [
        "### 2. Tanh\n",
        "- tanh 함수는 함수의 중심값을 0으로 옮겨 sigmoid의 최적화 과정이 느려지는 문제를 해결.\n",
        "- **vanishing gradient** 문제 존재."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlqPElDVhxAV"
      },
      "source": [
        "### 3. ReLU\n",
        "- sigmoid, tanh 함수에 비해 학습이 빠름.\n",
        "- 연산 비용이 크지 않고, 구현이 매우 간단하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt9kJVJbh2fE"
      },
      "source": [
        "#### 참고 자료\n",
        "- [딥러닝에서 사용하는 활성화 함수](https://reniew.github.io/12/)\n",
        "- [Activation Function](https://pozalabs.github.io/Activation_Function/)\n",
        "- [위키독스: 비선형 활성화 함수](https://wikidocs.net/60683)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU627AF4iPb_"
      },
      "source": [
        "# 단일 레이어 구현 함수\n",
        "def affine_layer_forward(X, W, b):\n",
        "    y = np.dot(X, W) + b\n",
        "    cache = (X, W, b)\n",
        "    return y, cache"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-BzJpPYiXrO",
        "outputId": "43184212-34d5-44d0-bc70-5b90ac3fd0dd"
      },
      "source": [
        "input_size = 784\n",
        "hidden_size = 50\n",
        "output_size = 10\n",
        "\n",
        "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros(hidden_size)\n",
        "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros(output_size)\n",
        "\n",
        "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
        "z1 = sigmoid(a1)\n",
        "a2, cache2 = affine_layer_forward(z1, W2, b2) # z1이 다시 두 번째 레이어의 입력이 된다\n",
        "\n",
        "print(a2[0]) # 최종 출력이 output_size 만큼의 벡터가 되었다"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.18749483  0.52671038  0.04811686  0.69348891  0.65440072  0.63875405\n",
            "  0.93367759  0.17532607  0.63499428 -0.32050926]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2X35-whjHsB"
      },
      "source": [
        "# softmax 함수\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71mO-U1GkiWJ",
        "outputId": "cbde308c-d661-4e6e-8ba1-67de7d0e2414"
      },
      "source": [
        "y_hat = softmax(a2)\n",
        "y_hat[0] # 10개의 숫자 중 하나일 확률"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.05267469, 0.10759135, 0.06666944, 0.12711838, 0.12224541,\n",
              "       0.12034756, 0.16162962, 0.07571346, 0.11989593, 0.04611418])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C012-jTknTf"
      },
      "source": [
        "### 손실함수\n",
        "\n",
        "#### 평균제곱오차(MSE: Mean Square Error)\n",
        "#### 교차 엔트로피(Cross Entropy)\n",
        "#### 참고 자료\n",
        "- [Understanding different Loss Functions for Neural Networks.](https://towardsdatascience.com/understanding-different-loss-functions-for-neural-networks-dd1ed0274718)\n",
        "- [손실함수(Loss Function)](http://www.gisdeveloper.co.kr/?p=7631)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30n3b9ZKk_9v",
        "outputId": "bd8cfea7-2435-4714-8721-bde1362ee602"
      },
      "source": [
        "# 정답 라벨을 One-hot 인코딩하는 함수\n",
        "def _change_one_hot_label(X, num_category):\n",
        "    T = np.zeros((X.size, num_category))\n",
        "    for idx, row in enumerate(T):\n",
        "        row[X[idx]] = 1\n",
        "    return T\n",
        "\n",
        "Y_digit = y_train[:5]\n",
        "t = _change_one_hot_label(Y_digit, 10)\n",
        "t # 정답 라벨의 One-hot 인코딩"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ld1zSM8luyA",
        "outputId": "732fc408-276b-4733-cc66-c37a0d989c26"
      },
      "source": [
        "print(y_hat[0])\n",
        "print(t[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.05267469 0.10759135 0.06666944 0.12711838 0.12224541 0.12034756\n",
            " 0.16162962 0.07571346 0.11989593 0.04611418]\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxA9ofmQlyKl",
        "outputId": "ccf502e2-e177-4247-ca2a-b32f5b07350a"
      },
      "source": [
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim ==1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
        "\n",
        "Loss = cross_entropy_error(y_hat, t)\n",
        "Loss"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5852455568289363"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H29MZrOWnXOc"
      },
      "source": [
        "## 15-4. 경사하강법\n",
        "\n",
        "- [[ ML ] 모두를 위한 TensorFlow (3) Gradient descent algorithm 기본](https://medium.com/@peteryun/ml-%EB%AA%A8%EB%91%90%EB%A5%BC-%EC%9C%84%ED%95%9C-tensorflow-3-gradient-descent-algorithm-%EA%B8%B0%EB%B3%B8-c0688208fc59)\n",
        "- [[머신러닝] lec 7-1 : 학습 Learning rate, Overfitting, 그리고 일반화](https://aileen93.tistory.com/71)\n",
        "- [가중치 초기화 (Weight Initialization)](https://reniew.github.io/13/)\n",
        "- [Classification and Loss Evaluation - Softmax and Cross Entropy Loss](https://deepnotes.io/softmax-crossentropy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Caesf5PQpYZV",
        "outputId": "f50331c4-6003-401f-977b-aa9f6bf43420"
      },
      "source": [
        "batch_num = y_hat.shape[0]\n",
        "dy = (y_hat - t) / batch_num\n",
        "dy # softmax값의 출력으로 Loss를 미분한 값"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.01053494,  0.02151827,  0.01333389,  0.02542368,  0.02444908,\n",
              "        -0.17593049,  0.03232592,  0.01514269,  0.02397919,  0.00922284],\n",
              "       [-0.19199804,  0.01979818,  0.01417711,  0.02882605,  0.02195335,\n",
              "         0.02676875,  0.0274539 ,  0.02082192,  0.02260808,  0.0095907 ],\n",
              "       [ 0.00891656,  0.0184666 ,  0.0169811 ,  0.02314872, -0.17812648,\n",
              "         0.02979259,  0.03064169,  0.01955448,  0.01980502,  0.01081973],\n",
              "       [ 0.00945165, -0.18090059,  0.01346373,  0.02518908,  0.02206237,\n",
              "         0.02750219,  0.02686516,  0.02218515,  0.0234314 ,  0.01074987],\n",
              "       [ 0.00995296,  0.02164623,  0.0122209 ,  0.02263612,  0.02270527,\n",
              "         0.0259472 ,  0.03043491,  0.0224797 ,  0.02229931, -0.19032261]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiJhVx7Cpgyt",
        "outputId": "acdd12ac-b0ca-48c8-ce26-09b24e90e63f"
      },
      "source": [
        "dW2 = np.dot(z1.T, dy)\n",
        "dW2"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.06358718, -0.0883621 ,  0.03074016,  0.05597507, -0.00943171,\n",
              "        -0.01312346,  0.06499961,  0.04569905,  0.05049071, -0.07340016],\n",
              "       [-0.00354152, -0.00809688,  0.01783798,  0.02958066, -0.06598501,\n",
              "        -0.0204293 ,  0.03705108,  0.02399722,  0.02678487, -0.0371991 ],\n",
              "       [-0.13953288, -0.07537005,  0.04921683,  0.08823934, -0.05636151,\n",
              "         0.01185671,  0.10271765,  0.07233924,  0.07846951, -0.13157484],\n",
              "       [-0.06733841, -0.00107882,  0.03537477,  0.06410259, -0.01907823,\n",
              "        -0.08939943,  0.07746137,  0.05004268,  0.05810953, -0.10819604],\n",
              "       [-0.1273039 , -0.04417295,  0.03813584,  0.06995668, -0.01955229,\n",
              "        -0.09804377,  0.08019985,  0.05214234,  0.06168883, -0.01305063],\n",
              "       [-0.09098096, -0.07797502,  0.04643734,  0.08406593, -0.02804386,\n",
              "        -0.08203172,  0.09915687,  0.06621202,  0.07578346, -0.09262405],\n",
              "       [-0.12778561, -0.03209086,  0.0472985 ,  0.08472446, -0.05961241,\n",
              "        -0.07111219,  0.09961016,  0.06606868,  0.07527645, -0.08237719],\n",
              "       [-0.12976954, -0.07187125,  0.03772383,  0.06868131, -0.02974823,\n",
              "        -0.04085916,  0.07797149,  0.05317857,  0.06037377, -0.0256808 ],\n",
              "       [-0.09279134, -0.04645287,  0.03606089,  0.06480916, -0.03631335,\n",
              "        -0.0186477 ,  0.07604024,  0.05234133,  0.057894  , -0.09294036],\n",
              "       [-0.12284711, -0.04821943,  0.04920318,  0.08884002, -0.04185034,\n",
              "        -0.07527908,  0.10458302,  0.06981037,  0.07944705, -0.10368768],\n",
              "       [-0.10766937, -0.01396778,  0.03264723,  0.05927959, -0.02406027,\n",
              "        -0.01208009,  0.06934598,  0.04783627,  0.05266782, -0.10399941],\n",
              "       [-0.05770125, -0.05716579,  0.02528027,  0.0441543 , -0.05607585,\n",
              "        -0.01459097,  0.0511777 ,  0.0348892 ,  0.03896458, -0.0089322 ],\n",
              "       [-0.09560773, -0.04771703,  0.04589386,  0.08229197, -0.04808511,\n",
              "        -0.08121172,  0.09763589,  0.06454851,  0.07387661, -0.09162524],\n",
              "       [-0.13969311, -0.02369548,  0.0483495 ,  0.0868305 , -0.05735752,\n",
              "        -0.06540812,  0.10199611,  0.06794974,  0.07705795, -0.09602956],\n",
              "       [-0.12916365, -0.05641464,  0.05405783,  0.09700356, -0.05863905,\n",
              "        -0.07651273,  0.11436056,  0.07640624,  0.08669809, -0.10779621],\n",
              "       [-0.03012703, -0.09241197,  0.01971295,  0.03664146,  0.01317715,\n",
              "        -0.02427266,  0.04190085,  0.02944637,  0.03335633, -0.02742345],\n",
              "       [-0.07826289, -0.06695017,  0.03637383,  0.06685743,  0.00194615,\n",
              "        -0.04936695,  0.07854832,  0.05358233,  0.0605125 , -0.10324054],\n",
              "       [-0.11007938, -0.05294179,  0.04639787,  0.08353018, -0.043871  ,\n",
              "        -0.07004359,  0.09836637,  0.06568987,  0.0747368 , -0.09178532],\n",
              "       [-0.07197981, -0.05286507,  0.02783268,  0.04839366, -0.06624558,\n",
              "         0.02132141,  0.05625872,  0.03979622,  0.0426421 , -0.04515432],\n",
              "       [-0.13820974, -0.08505394,  0.04562306,  0.08372545, -0.01388579,\n",
              "        -0.05775907,  0.09617718,  0.06557096,  0.07445113, -0.07063923],\n",
              "       [-0.07258831, -0.03220448,  0.02853125,  0.05221236, -0.00757439,\n",
              "        -0.04423241,  0.06134075,  0.04126108,  0.0469072 , -0.07365303],\n",
              "       [-0.14133329, -0.04440797,  0.0486216 ,  0.0871705 , -0.06192409,\n",
              "        -0.05892128,  0.10169282,  0.06816466,  0.07717013, -0.07623308],\n",
              "       [-0.10982081, -0.07877196,  0.03804815,  0.06940159, -0.02126705,\n",
              "        -0.05527682,  0.07962148,  0.05388329,  0.06161209, -0.03742996],\n",
              "       [-0.04124004, -0.08747108,  0.03578493,  0.06176654, -0.08170122,\n",
              "         0.01068792,  0.07371311,  0.05157641,  0.05564738, -0.07876397],\n",
              "       [-0.13500433, -0.08851739,  0.0333629 ,  0.06326419,  0.02810578,\n",
              "        -0.04361149,  0.07027842,  0.04881886,  0.05583409, -0.03253104],\n",
              "       [-0.07916748, -0.02558577,  0.02789184,  0.04836318, -0.07327772,\n",
              "        -0.00524502,  0.05651747,  0.0384447 ,  0.04235241, -0.03029362],\n",
              "       [-0.05106301, -0.0452919 ,  0.02677434,  0.04801843, -0.02592969,\n",
              "        -0.03563924,  0.05674036,  0.03821891,  0.04321752, -0.05504572],\n",
              "       [-0.07165822, -0.05640927,  0.03518224,  0.06192767, -0.06555214,\n",
              "        -0.05725873,  0.07278616,  0.04799833,  0.05506198, -0.02207802],\n",
              "       [-0.11281655, -0.02947927,  0.03621215,  0.06393696, -0.06963792,\n",
              "        -0.00565639,  0.07447973,  0.05112687,  0.05620087, -0.06436644],\n",
              "       [-0.09663815, -0.03765721,  0.04053038,  0.07221621, -0.05713224,\n",
              "        -0.06453474,  0.08517326,  0.0563165 ,  0.06432268, -0.06259667],\n",
              "       [-0.06260146, -0.0895521 ,  0.03000862,  0.05433527, -0.01798477,\n",
              "        -0.02169946,  0.06283181,  0.04368857,  0.04878978, -0.04781627],\n",
              "       [-0.10716477, -0.02644635,  0.03316021,  0.05941554, -0.04308853,\n",
              "        -0.00149784,  0.06918576,  0.04800072,  0.05250502, -0.08406975],\n",
              "       [-0.11112668, -0.01624448,  0.02449817,  0.04528749, -0.00831853,\n",
              "        -0.0087095 ,  0.05126197,  0.03564708,  0.03956086, -0.05185639],\n",
              "       [-0.12342414,  0.00915089,  0.03489403,  0.06168093, -0.06818202,\n",
              "        -0.00018174,  0.07237706,  0.04953055,  0.05406512, -0.08991068],\n",
              "       [-0.08897102, -0.02932063,  0.04415927,  0.07838221, -0.0628137 ,\n",
              "        -0.05936067,  0.09377685,  0.06234867,  0.0703643 , -0.10856528],\n",
              "       [-0.05292947, -0.03348638,  0.03848844,  0.0681525 , -0.05015102,\n",
              "        -0.04260783,  0.08266228,  0.05549478,  0.06191038, -0.12753369],\n",
              "       [-0.01526164, -0.03689445,  0.03031643,  0.05183486, -0.07400968,\n",
              "         0.00196152,  0.06405938,  0.04400562,  0.04734375, -0.11335579],\n",
              "       [-0.05249212, -0.07458459,  0.04415918,  0.07710522, -0.08472381,\n",
              "        -0.06317057,  0.09251209,  0.06135471,  0.06953038, -0.06969049],\n",
              "       [-0.01247148, -0.03497612,  0.02421253,  0.04233784, -0.03790402,\n",
              "        -0.01954823,  0.05201696,  0.03523807,  0.03886553, -0.08777108],\n",
              "       [-0.11605595, -0.01778314,  0.03336965,  0.06155617, -0.00384523,\n",
              "        -0.02527306,  0.07159704,  0.049225  ,  0.05480096, -0.10759144],\n",
              "       [-0.03537421, -0.09096907,  0.04062907,  0.07217435, -0.04209131,\n",
              "        -0.0383938 ,  0.08685567,  0.0593081 ,  0.06597773, -0.11811653],\n",
              "       [-0.1044653 , -0.02244784,  0.03343722,  0.06101881, -0.01899994,\n",
              "        -0.05213654,  0.07112508,  0.04752052,  0.05422488, -0.06927689],\n",
              "       [-0.04421518, -0.07376506,  0.0301388 ,  0.05437174, -0.02039555,\n",
              "        -0.06959294,  0.06398882,  0.04219491,  0.04915434, -0.03187988],\n",
              "       [-0.05700583, -0.03826991,  0.03104649,  0.05423925, -0.06005151,\n",
              "         0.01023524,  0.06493742,  0.04529478,  0.04871372, -0.09913964],\n",
              "       [-0.04333887, -0.07716592,  0.01861919,  0.03457017,  0.00843553,\n",
              "        -0.00635768,  0.03920248,  0.02812052,  0.03114817, -0.03323359],\n",
              "       [-0.14552562, -0.04862028,  0.03984055,  0.07377886, -0.00467377,\n",
              "        -0.07206941,  0.08419798,  0.05637376,  0.06512732, -0.0484294 ],\n",
              "       [-0.11244252, -0.0572627 ,  0.03397975,  0.06308137,  0.00545613,\n",
              "        -0.03535149,  0.07234056,  0.04987199,  0.05618314, -0.07585622],\n",
              "       [-0.10033241, -0.04834848,  0.03179855,  0.05700902, -0.03958632,\n",
              "         0.01043712,  0.06584681,  0.04645942,  0.05038732, -0.07367102],\n",
              "       [-0.13588012, -0.02414064,  0.04246708,  0.0743357 , -0.10195785,\n",
              "        -0.04539183,  0.08632119,  0.05714235,  0.06477509, -0.01767098],\n",
              "       [-0.12397913, -0.07335835,  0.05074329,  0.0910197 , -0.05584882,\n",
              "        -0.05092855,  0.10664375,  0.07233593,  0.08123689, -0.09786472]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPHQmvSrpmv9"
      },
      "source": [
        "dW2 = np.dot(z1.T, dy)\n",
        "db2 = np.sum(dy, axis=0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5UckOQiptfs"
      },
      "source": [
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgWbYSnZpxdr"
      },
      "source": [
        "dz1 = np.dot(dy, W2.T)\n",
        "da1 = sigmoid_grad(a1) * dz1\n",
        "dW1 = np.dot(X.T, da1)\n",
        "db1 = np.sum(dz1, axis=0)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOUgyuHXsRl0"
      },
      "source": [
        "learning_rate = 0.1 # 파라미터 업데이트하는데 learning rate 고려\n",
        "\n",
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
        "    W1 = W1 - learning_rate*dW1\n",
        "    b1 = b1 - learning_rate*db1\n",
        "    W2 = W2 - learning_rate*dW2\n",
        "    b2 = b2 - learning_rate*db2\n",
        "    return W1, b1, W2, b2"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scK8d4QrsZi7"
      },
      "source": [
        "## 15-5. 오차역전파법이란?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TzwC6GIsxkQ"
      },
      "source": [
        "def affine_layer_backward(dy, cache):\n",
        "    X, W, b = cache\n",
        "    dX = np.dot(dy, W.T)\n",
        "    dW = np.dot(X.T, dy)\n",
        "    db = np.sum(dy, axis=0)\n",
        "    return dX, dW, db"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4SaaAD8sx2q",
        "outputId": "264508dd-ba14-441c-88dc-27ab5ccd5a2f"
      },
      "source": [
        "# 파라미터 초기화\n",
        "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros(hidden_size)\n",
        "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros(output_size)\n",
        "\n",
        "# Forward Propagation\n",
        "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
        "z1 = sigmoid(a1)\n",
        "a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
        "\n",
        "# 추론과 오차(Loss) 계산\n",
        "y_hat = softmax(a2)\n",
        "t = _change_one_hot_label(Y_digit, 10)   # 정답 One-hot 인코딩\n",
        "Loss = cross_entropy_error(y_hat, t)\n",
        "\n",
        "print(y_hat)\n",
        "print(t)\n",
        "print('Loss: ', Loss)\n",
        "        \n",
        "dy = (y_hat - t) / X.shape[0]\n",
        "dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
        "da1 = sigmoid_grad(a1) * dz1\n",
        "dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
        "\n",
        "# 경사하강법을 통한 파라미터 업데이트    \n",
        "learning_rate = 0.1\n",
        "W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.07854101 0.08583584 0.16195867 0.03628619 0.14363453 0.14344054\n",
            "  0.08655567 0.13248348 0.04979976 0.08146432]\n",
            " [0.08929667 0.07754062 0.14601446 0.04120935 0.16415818 0.13229204\n",
            "  0.07506148 0.13046724 0.04908107 0.0948789 ]\n",
            " [0.0741119  0.0724017  0.14967661 0.03875492 0.13515956 0.16858943\n",
            "  0.08003447 0.14433994 0.05161978 0.08531168]\n",
            " [0.08529008 0.09120665 0.18317372 0.03813327 0.14060426 0.11646996\n",
            "  0.0772531  0.14050043 0.04609904 0.08126948]\n",
            " [0.09147438 0.07906725 0.18298582 0.03998373 0.13211183 0.11414967\n",
            "  0.08231523 0.14287568 0.04860581 0.0864306 ]]\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Loss:  2.2403931915505937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zuPOcXZs0QK"
      },
      "source": [
        "## 15-6. 모델 학습 Step-by-Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeQ-3mBFs5eH"
      },
      "source": [
        "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros(hidden_size)\n",
        "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros(output_size)\n",
        "\n",
        "def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):\n",
        "    a1, cache1 = affine_layer_forward(X, W1, b1)\n",
        "    z1 = sigmoid(a1)\n",
        "    a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
        "    y_hat = softmax(a2)\n",
        "    t = _change_one_hot_label(Y, 10)\n",
        "    Loss = cross_entropy_error(y_hat, t)\n",
        "\n",
        "    if verbose:\n",
        "        print('---------')\n",
        "        print(y_hat)\n",
        "        print(t)\n",
        "        print('Loss: ', Loss)\n",
        "        \n",
        "    dy = (y_hat - t) / X.shape[0]\n",
        "    dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
        "    da1 = sigmoid_grad(a1) * dz1\n",
        "    dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
        "    \n",
        "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
        "    \n",
        "    return W1, b1, W2, b2, Loss"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xntfNDcZs6vT",
        "outputId": "29cfbd5b-302b-45d0-897a-c6318abef368"
      },
      "source": [
        "X = x_train_reshaped[:5]\n",
        "Y = y_train[:5]\n",
        "\n",
        "# train_step을 다섯 번 반복 돌립니다.\n",
        "for i in range(5):\n",
        "    W1, b1, W2, b2, _ = train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------\n",
            "[[0.10751559 0.08953856 0.12133613 0.05899732 0.1883878  0.07261783\n",
            "  0.08164345 0.13325444 0.06017146 0.08653743]\n",
            " [0.1154696  0.08658154 0.11797907 0.06141441 0.14453371 0.10248901\n",
            "  0.08397174 0.14921289 0.0611604  0.07718763]\n",
            " [0.12322821 0.07252239 0.11183499 0.05027784 0.18469764 0.1017306\n",
            "  0.06775562 0.13777928 0.05961889 0.09055457]\n",
            " [0.10763685 0.08898321 0.09976611 0.06538768 0.18751228 0.09320573\n",
            "  0.08291292 0.12495163 0.06141159 0.08823199]\n",
            " [0.10844757 0.07460602 0.13157959 0.06320404 0.16558187 0.08236353\n",
            "  0.08821284 0.1332635  0.05970736 0.09303368]]\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Loss:  2.25288584731265\n",
            "---------\n",
            "[[0.12403336 0.10805693 0.09791501 0.05248309 0.19404796 0.09144132\n",
            "  0.07043472 0.10571426 0.05341281 0.10246055]\n",
            " [0.13837199 0.10394777 0.09547357 0.05437695 0.14726454 0.12345676\n",
            "  0.07220959 0.11877882 0.05412003 0.09199999]\n",
            " [0.13768286 0.08621492 0.09021292 0.04432964 0.19872833 0.11942875\n",
            "  0.05772962 0.10648521 0.05293617 0.10625158]\n",
            " [0.12151686 0.11147272 0.08013996 0.05766119 0.19078852 0.10972579\n",
            "  0.0712821  0.09802345 0.05456378 0.10482562]\n",
            " [0.12458371 0.09057492 0.10793291 0.05607658 0.17130598 0.09759934\n",
            "  0.07637229 0.10522727 0.05360723 0.11671977]]\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Loss:  2.0655277631925113\n",
            "---------\n",
            "[[0.13632162 0.12394793 0.08046567 0.0463563  0.19415023 0.10943942\n",
            "  0.06074045 0.0863718  0.04698923 0.11521735]\n",
            " [0.15773543 0.11843249 0.07863254 0.04776397 0.14560933 0.14110111\n",
            "  0.06199232 0.09723316 0.04741683 0.10408283]\n",
            " [0.14705208 0.09781991 0.07429232 0.03890651 0.20810436 0.13381775\n",
            "  0.04930172 0.08499659 0.04672071 0.11898805]\n",
            " [0.13099503 0.13309969 0.06569185 0.05059683 0.18889634 0.12314431\n",
            "  0.06139974 0.07938154 0.04814315 0.11865152]\n",
            " [0.13620954 0.10448112 0.09003993 0.04933858 0.17193973 0.10982303\n",
            "  0.06601848 0.08546355 0.04765655 0.13902947]]\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Loss:  1.9237324401758844\n",
            "---------\n",
            "[[0.1452071  0.13699987 0.06729541 0.04099714 0.19203384 0.12630475\n",
            "  0.05269366 0.07223447 0.041341   0.12489277]\n",
            " [0.17419043 0.12998924 0.06592333 0.04201622 0.14232388 0.15547867\n",
            "  0.05353468 0.0814391  0.04154886 0.1135556 ]\n",
            " [0.15264508 0.10730009 0.06242113 0.03427264 0.21580691 0.14511612\n",
            "  0.04247028 0.06965672 0.04134627 0.12896477]\n",
            " [0.1370661  0.15341542 0.05487471 0.04451656 0.18507789 0.13357615\n",
            "  0.05328732 0.06590906 0.04253258 0.12974418]\n",
            " [0.1442194  0.11611956 0.07637302 0.04342906 0.17042652 0.11911477\n",
            "  0.05736939 0.07099487 0.04233886 0.15961456]]\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Loss:  1.811926854239399\n",
            "---------\n",
            "[[0.15161335 0.14731498 0.05719076 0.03644738 0.18936662 0.14199354\n",
            "  0.0461037  0.06158875 0.03653223 0.13184869]\n",
            " [0.18856779 0.13884162 0.05619338 0.03717494 0.13873174 0.16696453\n",
            "  0.04664595 0.06954451 0.03658448 0.12075107]\n",
            " [0.15562607 0.11477312 0.05339999 0.03040303 0.2233887  0.15372702\n",
            "  0.03697549 0.05834572 0.03682137 0.13653949]\n",
            " [0.1407245  0.17225241 0.04662014 0.03940692 0.18089347 0.14139449\n",
            "  0.0466912  0.05585116 0.03776893 0.13839678]\n",
            " [0.14958765 0.1255478  0.06578629 0.03840432 0.16826734 0.12582454\n",
            "  0.05025677 0.06009473 0.03775819 0.17847237]]\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Loss:  1.7202458542019656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsFkxArXtGzg"
      },
      "source": [
        "## 15-7. 추론 과정 구현과 정확도(Accuracy) 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jQzSKaktMvB"
      },
      "source": [
        "def predict(W1, b1, W2, b2, X):\n",
        "    a1 = np.dot(X, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    y = softmax(a2)\n",
        "\n",
        "    return y"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nqQVgjEtPE1",
        "outputId": "42d75389-ed96-4dbe-8fa0-f35731ca5298"
      },
      "source": [
        "# X = x_train[:100] 에 대해 모델 추론을 시도합니다. \n",
        "X = x_train_reshaped[:100]\n",
        "Y = y_test[:100]\n",
        "result = predict(W1, b1, W2, b2, X)\n",
        "result[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.15631047, 0.15517733, 0.04930513, 0.03262571, 0.18684658,\n",
              "       0.15661544, 0.04071253, 0.0533748 , 0.0324866 , 0.13654542])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV_LUEsrtQHn"
      },
      "source": [
        "def accuracy(W1, b1, W2, b2, x, y):\n",
        "    y_hat = predict(W1, b1, W2, b2, x)\n",
        "    y_hat = np.argmax(y_hat, axis=1)\n",
        "\n",
        "    accuracy = np.sum(y_hat == y) / float(x.shape[0])\n",
        "    return accuracy"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQjIsRa8tRLc",
        "outputId": "ac814b82-d277-4c42-a155-beeea03fa991"
      },
      "source": [
        "acc = accuracy(W1, b1, W2, b2, X, Y)\n",
        "\n",
        "t = _change_one_hot_label(Y, 10)\n",
        "print(result[0])\n",
        "print(t[0])\n",
        "print(acc)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.15631047 0.15517733 0.04930513 0.03262571 0.18684658 0.15661544\n",
            " 0.04071253 0.0533748  0.0324866  0.13654542]\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "0.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPf8XEFTtSCH"
      },
      "source": [
        "## 15-8. 전체 학습 사이클 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLevUaVttWLO"
      },
      "source": [
        "def init_params(input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "\n",
        "    W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    b1 = np.zeros(hidden_size)\n",
        "    W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    b2 = np.zeros(output_size)\n",
        "\n",
        "    print(W1.shape)\n",
        "    print(b1.shape)\n",
        "    print(W2.shape)\n",
        "    print(b2.shape)\n",
        "    \n",
        "    return W1, b1, W2, b2"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJvUDcYotXSX",
        "outputId": "4904437c-8338-49b2-f95d-b52f17d594eb"
      },
      "source": [
        "# 하이퍼파라미터\n",
        "iters_num = 50000  # 반복 횟수를 적절히 설정한다.\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100   # 미니배치 크기\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# 1에폭당 반복 수\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "W1, b1, W2, b2 = init_params(784, 50, 10)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    # 미니배치 획득\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train_reshaped[batch_mask]\n",
        "    y_batch = y_train[batch_mask]\n",
        "    \n",
        "    W1, b1, W2, b2, Loss = train_step(x_batch, y_batch, W1, b1, W2, b2, learning_rate=0.1, verbose=False)\n",
        "\n",
        "    # 학습 경과 기록\n",
        "    train_loss_list.append(Loss)\n",
        "    \n",
        "    # 1에폭당 정확도 계산\n",
        "    if i % iter_per_epoch == 0:\n",
        "        print('Loss: ', Loss)\n",
        "        train_acc = accuracy(W1, b1, W2, b2, x_train_reshaped, y_train)\n",
        "        test_acc = accuracy(W1, b1, W2, b2, x_test_reshaped, y_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 50)\n",
            "(50,)\n",
            "(50, 10)\n",
            "(10,)\n",
            "Loss:  2.304916422295134\n",
            "train acc, test acc | 0.10218333333333333, 0.101\n",
            "Loss:  0.8743931164224138\n",
            "train acc, test acc | 0.7884833333333333, 0.7905\n",
            "Loss:  0.4243152357162353\n",
            "train acc, test acc | 0.87615, 0.8826\n",
            "Loss:  0.4261118386612813\n",
            "train acc, test acc | 0.8974166666666666, 0.9022\n",
            "Loss:  0.27563586170171583\n",
            "train acc, test acc | 0.9083833333333333, 0.9098\n",
            "Loss:  0.23214605122402215\n",
            "train acc, test acc | 0.9149666666666667, 0.9179\n",
            "Loss:  0.34510938148487214\n",
            "train acc, test acc | 0.9200333333333334, 0.9233\n",
            "Loss:  0.28068647988640405\n",
            "train acc, test acc | 0.9239833333333334, 0.9244\n",
            "Loss:  0.2778968612347907\n",
            "train acc, test acc | 0.9286833333333333, 0.9296\n",
            "Loss:  0.12112261164424493\n",
            "train acc, test acc | 0.93165, 0.9337\n",
            "Loss:  0.27210503358485244\n",
            "train acc, test acc | 0.9345833333333333, 0.9355\n",
            "Loss:  0.24932533219533226\n",
            "train acc, test acc | 0.9376333333333333, 0.9371\n",
            "Loss:  0.23033720822430495\n",
            "train acc, test acc | 0.9399166666666666, 0.9394\n",
            "Loss:  0.18758175424755155\n",
            "train acc, test acc | 0.9423833333333334, 0.9411\n",
            "Loss:  0.20961124517460028\n",
            "train acc, test acc | 0.9433166666666667, 0.9436\n",
            "Loss:  0.13666858759181513\n",
            "train acc, test acc | 0.9460333333333333, 0.9444\n",
            "Loss:  0.1338978502791449\n",
            "train acc, test acc | 0.9472666666666667, 0.9459\n",
            "Loss:  0.073359782919483\n",
            "train acc, test acc | 0.9486833333333333, 0.9465\n",
            "Loss:  0.2188364717250057\n",
            "train acc, test acc | 0.9501333333333334, 0.9477\n",
            "Loss:  0.22315431231019078\n",
            "train acc, test acc | 0.9512666666666667, 0.949\n",
            "Loss:  0.29797048264560966\n",
            "train acc, test acc | 0.95205, 0.9492\n",
            "Loss:  0.2042182762433344\n",
            "train acc, test acc | 0.9536166666666667, 0.951\n",
            "Loss:  0.1465123788652029\n",
            "train acc, test acc | 0.9551666666666667, 0.9521\n",
            "Loss:  0.15371327160096712\n",
            "train acc, test acc | 0.95595, 0.9531\n",
            "Loss:  0.13334551244392895\n",
            "train acc, test acc | 0.95695, 0.9534\n",
            "Loss:  0.18696802046410632\n",
            "train acc, test acc | 0.9580333333333333, 0.9544\n",
            "Loss:  0.14585085245849944\n",
            "train acc, test acc | 0.9592166666666667, 0.9555\n",
            "Loss:  0.11497098105176588\n",
            "train acc, test acc | 0.9599333333333333, 0.9567\n",
            "Loss:  0.09858925065544419\n",
            "train acc, test acc | 0.9607666666666667, 0.9564\n",
            "Loss:  0.19138840988151318\n",
            "train acc, test acc | 0.9614666666666667, 0.9568\n",
            "Loss:  0.14730601806611865\n",
            "train acc, test acc | 0.9626833333333333, 0.9575\n",
            "Loss:  0.23020457744863393\n",
            "train acc, test acc | 0.9632166666666667, 0.9587\n",
            "Loss:  0.09815590445851574\n",
            "train acc, test acc | 0.9647833333333333, 0.9596\n",
            "Loss:  0.0970915618536823\n",
            "train acc, test acc | 0.9650666666666666, 0.9595\n",
            "Loss:  0.12200224798876906\n",
            "train acc, test acc | 0.9652833333333334, 0.9609\n",
            "Loss:  0.13419001706898692\n",
            "train acc, test acc | 0.9660333333333333, 0.961\n",
            "Loss:  0.15357422987375313\n",
            "train acc, test acc | 0.9667666666666667, 0.9625\n",
            "Loss:  0.1822212880828496\n",
            "train acc, test acc | 0.9671666666666666, 0.9622\n",
            "Loss:  0.07071743483352937\n",
            "train acc, test acc | 0.9678666666666667, 0.9628\n",
            "Loss:  0.08373831867751452\n",
            "train acc, test acc | 0.96835, 0.9632\n",
            "Loss:  0.09256343767929065\n",
            "train acc, test acc | 0.9691166666666666, 0.963\n",
            "Loss:  0.08229618471059451\n",
            "train acc, test acc | 0.96945, 0.9641\n",
            "Loss:  0.09413677597452018\n",
            "train acc, test acc | 0.9701166666666666, 0.9644\n",
            "Loss:  0.10352093865645483\n",
            "train acc, test acc | 0.9704333333333334, 0.9645\n",
            "Loss:  0.11357603123805518\n",
            "train acc, test acc | 0.9711166666666666, 0.9642\n",
            "Loss:  0.07293496114806511\n",
            "train acc, test acc | 0.9716833333333333, 0.9657\n",
            "Loss:  0.1339197422079295\n",
            "train acc, test acc | 0.9722166666666666, 0.9652\n",
            "Loss:  0.2338908284132539\n",
            "train acc, test acc | 0.97265, 0.9651\n",
            "Loss:  0.0636088572366687\n",
            "train acc, test acc | 0.9731333333333333, 0.9655\n",
            "Loss:  0.17065453413141937\n",
            "train acc, test acc | 0.9735333333333334, 0.9664\n",
            "Loss:  0.060602924579589414\n",
            "train acc, test acc | 0.9734833333333334, 0.9657\n",
            "Loss:  0.0603259651529239\n",
            "train acc, test acc | 0.9743, 0.9668\n",
            "Loss:  0.16146249929369622\n",
            "train acc, test acc | 0.9742, 0.9665\n",
            "Loss:  0.06962792605149991\n",
            "train acc, test acc | 0.9747166666666667, 0.9672\n",
            "Loss:  0.08902316245074599\n",
            "train acc, test acc | 0.97505, 0.9672\n",
            "Loss:  0.07636617486459928\n",
            "train acc, test acc | 0.9753833333333334, 0.9669\n",
            "Loss:  0.0586974450351636\n",
            "train acc, test acc | 0.9762, 0.9678\n",
            "Loss:  0.10340650631825093\n",
            "train acc, test acc | 0.9763833333333334, 0.9682\n",
            "Loss:  0.13505052844404047\n",
            "train acc, test acc | 0.97665, 0.968\n",
            "Loss:  0.09828582519640268\n",
            "train acc, test acc | 0.9768833333333333, 0.9682\n",
            "Loss:  0.10661072292097776\n",
            "train acc, test acc | 0.9771, 0.9688\n",
            "Loss:  0.08098566475366545\n",
            "train acc, test acc | 0.9776333333333334, 0.9682\n",
            "Loss:  0.06663507716198556\n",
            "train acc, test acc | 0.9776333333333334, 0.9687\n",
            "Loss:  0.0766521910964074\n",
            "train acc, test acc | 0.97795, 0.9683\n",
            "Loss:  0.09512709271918379\n",
            "train acc, test acc | 0.9783666666666667, 0.9694\n",
            "Loss:  0.11679183532107837\n",
            "train acc, test acc | 0.9785666666666667, 0.9693\n",
            "Loss:  0.07608189372007726\n",
            "train acc, test acc | 0.979, 0.9706\n",
            "Loss:  0.07883465638600594\n",
            "train acc, test acc | 0.9791833333333333, 0.9692\n",
            "Loss:  0.08361847408065644\n",
            "train acc, test acc | 0.9795166666666667, 0.9701\n",
            "Loss:  0.07926453418993386\n",
            "train acc, test acc | 0.9799666666666667, 0.9704\n",
            "Loss:  0.04830425546435805\n",
            "train acc, test acc | 0.9798166666666667, 0.9713\n",
            "Loss:  0.06169233025283525\n",
            "train acc, test acc | 0.9803666666666667, 0.9708\n",
            "Loss:  0.13023864955877848\n",
            "train acc, test acc | 0.9806666666666667, 0.9701\n",
            "Loss:  0.11412245441633803\n",
            "train acc, test acc | 0.9805166666666667, 0.9715\n",
            "Loss:  0.109715608718284\n",
            "train acc, test acc | 0.9806166666666667, 0.9714\n",
            "Loss:  0.05800602417771726\n",
            "train acc, test acc | 0.9810833333333333, 0.9717\n",
            "Loss:  0.08729156397037509\n",
            "train acc, test acc | 0.9814833333333334, 0.9715\n",
            "Loss:  0.05928789274870807\n",
            "train acc, test acc | 0.9816333333333334, 0.9714\n",
            "Loss:  0.08538115372236293\n",
            "train acc, test acc | 0.9818666666666667, 0.9719\n",
            "Loss:  0.08395431496162399\n",
            "train acc, test acc | 0.9815666666666667, 0.9719\n",
            "Loss:  0.0270715630180103\n",
            "train acc, test acc | 0.9823, 0.9727\n",
            "Loss:  0.12693556872867784\n",
            "train acc, test acc | 0.98235, 0.9722\n",
            "Loss:  0.04329931795835353\n",
            "train acc, test acc | 0.9825166666666667, 0.9714\n",
            "Loss:  0.06727230776093905\n",
            "train acc, test acc | 0.9825333333333334, 0.9718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "gPHhrsDztZUr",
        "outputId": "6a9f75cc-c42d-44b0-ca92-6c3fe35ad942"
      },
      "source": [
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize'] = 12, 6 \n",
        "\n",
        "# Accuracy 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label='train acc')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "u3T7VXtHtbV_",
        "outputId": "11f1470e-f2dc-4ed2-b6e1-1431a9ef0985"
      },
      "source": [
        "# Loss 그래프 그리기\n",
        "x = np.arange(len(train_loss_list))\n",
        "plt.plot(x, train_loss_list, label='train acc')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.ylim(0, 3.0)\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo-IuIWGtcTW"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}